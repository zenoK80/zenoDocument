---
title: "연관 사상 (Fully Associative Mapping)"
description: "가장 유연한 캐시 매핑 방식인 연관 사상의 원리와 교체 알고리즘을 상세히 학습합니다."
slug: "fully-associative-mapping"
sidebar_position: 3
---

# 연관 사상 (Fully Associative Mapping)

## 개념

연관 사상은 **가장 유연한** 캐시 매핑 방식입니다.  
주기억장치의 어떤 블록이든 **캐시의 어느 라인에나** 저장될 수 있습니다.

:::tip 핵심 아이디어
블록과 캐시 라인 사이에 고정된 매핑 규칙이 없습니다!
빈 라인이 있으면 아무 곳에나 저장 가능합니다.
:::

---

## 도서관 비유

연관 사상을 도서관에 비유하면:

```
📚 중앙 도서관: 1000권의 책 (주기억장치의 1000개 블록)
📖 개인 책상: 10개의 공간 (캐시의 10개 라인)

규칙: 없음! 아무 자리에나 자유롭게 배치 가능!

ISBN이 ...0으로 끝나는 책 → 아무 자리나 OK
ISBN이 ...1으로 끝나는 책 → 아무 자리나 OK
...

예시:
- 책 A (ISBN: 1234567890) → 3번 자리에 배치
- 책 B (ISBN: 9876543210) → 7번 자리에 배치
- 책 C (ISBN: 1111111110) → 1번 자리에 배치

완전히 자유롭습니다!
```

**단, 책을 찾으려면?**
```
모든 자리를 다 확인해야 합니다!
- 0번 자리 확인 → X
- 1번 자리 확인 → X
- 2번 자리 확인 → X
- 3번 자리 확인 → O 찾았다!

→ 찾는데 시간이 오래 걸립니다.
```

이것이 연관 사상의 **장점(유연성)**과 **단점(검색 시간)**입니다.

---

## 매핑 규칙

### 기본 원칙

```
매핑 규칙: 없음!

- 빈 라인이 있으면 → 아무 곳에나 저장
- 모든 라인이 가득 찼으면 → 교체 알고리즘으로 선택
```

### 시각화

```
주기억장치 (32개 블록)         캐시 (8개 라인)
┌─────────┐                   ┌──────┬──────────┐
│ 블록 0  │──┐                │Line 0│ Tag: 15  │ ← 블록 15
│ 블록 1  │  │  자유롭게      │Line 1│ Tag: 1   │ ← 블록 1
│ 블록 2  │  │  배치          │Line 2│ Tag: 23  │ ← 블록 23
│  ...    │  │                │Line 3│ Tag: 0   │ ← 블록 0
│ 블록 15 │──┴────────────────→│Line 4│ Tag: 7   │ ← 블록 7
│ 블록 16 │                   │Line 5│ Tag: 31  │ ← 블록 31
│  ...    │                   │Line 6│ Tag: 4   │ ← 블록 4
│ 블록 31 │───────────────────→│Line 7│ Tag: 12  │ ← 블록 12
└─────────┘                   └──────┴──────────┘

블록 0 → 라인 3
블록 1 → 라인 1
블록 15 → 라인 0
...
(규칙 없이 자유롭게 배치됨!)
```

---

## 주소 구조

연관 사상에서는 **인덱스가 필요 없습니다!**

### 주소 분해

```
직접 사상:
┌────────┬────────┬────────┐
│  태그   │ 인덱스  │ 오프셋  │
└────────┴────────┴────────┘

연관 사상:
┌──────────────────┬────────┐
│       태그        │ 오프셋  │
└──────────────────┴────────┘
     어떤 블록?      몇 번째?

※ 인덱스 없음 = 어디든 저장 가능!
```

### 각 부분의 크기 계산

**조건:**
- 전체 주소: 16비트
- 블록 크기: 4바이트 = 2²바이트
- 캐시 라인: 8개 (크기만 알면 됨, 인덱스 불필요)

**계산:**

```
1. 오프셋 비트 수
   = log₂(블록 크기)
   = log₂(4)
   = 2비트

2. 인덱스 비트 수
   = 0비트 (필요 없음!)

3. 태그 비트 수
   = 전체 비트 - 오프셋 비트
   = 16 - 2
   = 14비트  ← 직접 사상(11비트)보다 더 김!
```

**결과:**

```
┌───────────────────┬────────┐
│       태그         │ 오프셋  │
│      14비트        │  2비트  │
└───────────────────┴────────┘
```

### 구체적인 예시

주소 `0x1A4C` = `0001 1010 0100 1100`

```
주소: 0001 1010 0100 1100

분해:
┌─────────────────────┬────┐
│ 00 0110 1001 0011   │ 00 │
│        태그          │오프셋│
└─────────────────────┴────┘

10진수 변환:
- 태그: 00 0110 1001 0011 = 1683
- 오프셋: 00 = 0

해석:
- 블록 1683의 데이터를 요청
- 모든 캐시 라인을 검색하여 태그 1683을 찾음
- 블록의 0번째 바이트를 반환
```

---

## 동작 과정

### 전체 흐름도

```
CPU가 주소 요청
    ↓
┌─────────────────────┐
│ 1. 주소 분해        │
│  - 태그             │
│  - 오프셋           │
└──────┬──────────────┘
       ↓
┌─────────────────────┐
│ 2. 모든 캐시 라인을 │
│    병렬로 검색      │
│    (태그 비교)      │
└──────┬──────────────┘
       ↓
┌─────────────────────┐
│ 3. 태그 일치하는    │
│    라인 찾기        │
└──────┬──────────────┘
       ↓
    ┌──┴──┐
    │찾음?│
    └──┬──┘
   YES │ NO
       │  │
   ┌───▼──▼────────┐
   │ 4. Hit / Miss │
   └───────────────┘
       ↓
    (Miss인 경우)
┌─────────────────────┐
│ 5. 교체 알고리즘    │
│    실행             │
└─────────────────────┘
```

### 단계별 상세 설명

#### 1단계: 주소 분해

```c
unsigned int address = 0x1A4C;

// 연관 사상은 인덱스가 없음!
unsigned int offset = address & 0x0003;  // 하위 2비트
unsigned int tag = address >> 2;         // 상위 14비트

printf("주소: 0x%04X\n", address);
printf("  태그: %u\n", tag);      // 1683
printf("  오프셋: %u\n", offset);  // 0
```

#### 2단계: 병렬 검색

**핵심: 모든 캐시 라인을 동시에 검색합니다!**

```c
// 하드웨어에서는 병렬로 수행되지만,
// 소프트웨어 시뮬레이션에서는 순차적으로 검색

int found_line = -1;

// 모든 라인을 검색
for (int i = 0; i < CACHE_LINES; i++) {
    // 유효하고 태그가 일치하는 라인 찾기
    if (cache[i].valid && cache[i].tag == tag) {
        found_line = i;
        break;  // 찾으면 즉시 종료
    }
}
```

**하드웨어 구현 (병렬 비교):**

```
        요청한 태그 (1683)
              ↓
    ┌─────────┴─────────┐
    │                   │
    ▼                   ▼
┌───────┐           ┌───────┐
│ 비교기 │ ← Tag 0  │ 비교기 │ ← Tag 1
│   0   │           │   1   │
└───┬───┘           └───┬───┘
    │                   │
    ▼                   ▼
┌───────┐           ┌───────┐
│ 비교기 │ ← Tag 2  │ 비교기 │ ← Tag 3
│   2   │           │   3   │
└───┬───┘           └───┬───┘
    │                   │
    ▼                   ▼
  ... (모든 라인에 대해)

→ 8개의 비교기가 동시에 동작
→ 1 클럭 사이클에 모든 비교 완료
```

#### 3단계: Hit/Miss 처리

**캐시 Hit:**

```c
if (found_line != -1) {
    printf("✅ 캐시 Hit! (라인 %d)\n", found_line);
    
    // LRU 카운터 갱신
    cache[found_line].lru_counter = global_time++;
    
    // 데이터 반환
    return cache[found_line].data[offset];
}
```

**캐시 Miss:**

```c
else {
    printf("❌ 캐시 Miss!\n");
    
    // 빈 라인 찾기
    int empty_line = find_empty_line();
    
    if (empty_line != -1) {
        // 빈 라인에 저장
        printf("→ 빈 라인 %d에 저장\n", empty_line);
        cache[empty_line].valid = true;
        cache[empty_line].tag = tag;
        cache[empty_line].lru_counter = global_time++;
        fetch_from_memory(address, cache[empty_line].data);
    }
    else {
        // 교체 알고리즘 실행 (다음 섹션에서 자세히)
        int victim = select_victim();
        printf("→ 라인 %d 교체\n", victim);
        cache[victim].tag = tag;
        cache[victim].lru_counter = global_time++;
        fetch_from_memory(address, cache[victim].data);
    }
}
```

---

## 교체 알고리즘

캐시가 가득 찼을 때 어떤 블록을 내보낼지 결정하는 **가장 중요한 부분**입니다!

### 1. LRU (Least Recently Used)

**가장 오래 사용하지 않은 블록을 교체합니다.**

#### 개념

```
아이디어: 최근에 사용한 데이터는 곧 다시 사용될 가능성이 높다
         (시간 지역성)

예시:
시간  0   1   2   3   4   5
접근  A   B   C   D   E   A
LRU       A   B   C   D   E

시간 5에서:
- A는 방금 사용됨 (최신)
- E는 시간 4에 사용됨
- D는 시간 3에 사용됨
- C는 시간 2에 사용됨
- B는 시간 1에 사용됨 (가장 오래됨, LRU)

→ B를 교체!
```

#### 구현 방법

**방법 1: 타임스탬프 사용**

```c
typedef struct {
    bool valid;
    unsigned int tag;
    unsigned char data[BLOCK_SIZE];
    int last_used_time;  // 마지막 사용 시간
} CacheLine;

int global_time = 0;  // 전역 시간 카운터

// LRU 블록 찾기
int find_lru_line() {
    int lru_line = 0;
    int min_time = cache[0].last_used_time;
    
    // 가장 작은 타임스탬프를 가진 라인 찾기
    for (int i = 1; i < CACHE_LINES; i++) {
        if (cache[i].last_used_time < min_time) {
            min_time = cache[i].last_used_time;
            lru_line = i;
        }
    }
    
    return lru_line;
}

// 캐시 접근 시 타임스탬프 갱신
void update_lru(int line) {
    cache[line].last_used_time = global_time++;
}
```

**방법 2: LRU 스택 (더 정확하지만 복잡)**

```c
// LRU 스택: 가장 최근 사용한 라인이 맨 위
int lru_stack[CACHE_LINES];  // 라인 번호를 저장

// 초기화
void init_lru_stack() {
    for (int i = 0; i < CACHE_LINES; i++) {
        lru_stack[i] = i;
    }
}

// 라인 사용 시 스택 갱신
void update_stack(int line) {
    // 1. 스택에서 해당 라인 찾기
    int pos = -1;
    for (int i = 0; i < CACHE_LINES; i++) {
        if (lru_stack[i] == line) {
            pos = i;
            break;
        }
    }
    
    // 2. 찾은 라인을 맨 위로 이동
    for (int i = pos; i > 0; i--) {
        lru_stack[i] = lru_stack[i - 1];
    }
    lru_stack[0] = line;
}

// LRU 블록 찾기 (스택의 맨 아래)
int find_lru() {
    return lru_stack[CACHE_LINES - 1];
}
```

**예시 시뮬레이션:**

```
초기 상태:
스택: [0, 1, 2, 3, 4, 5, 6, 7]
      ↑ 최근        오래됨 ↑

라인 3 접근:
스택: [3, 0, 1, 2, 4, 5, 6, 7]
      ↑ 3이 맨 위로

라인 5 접근:
스택: [5, 3, 0, 1, 2, 4, 6, 7]
      ↑ 5가 맨 위로

라인 3 다시 접근:
스택: [3, 5, 0, 1, 2, 4, 6, 7]
      ↑ 3이 다시 맨 위로

교체 필요 시:
→ 스택 맨 아래의 7번 라인 교체!
```

#### 장점과 단점

**장점 ✅**
- 이론적으로 최적에 가까운 성능
- 시간 지역성을 잘 활용
- 실제로 가장 많이 사용되는 알고리즘

**단점 ❌**
- 구현이 복잡함 (타임스탬프 또는 스택 필요)
- 하드웨어 비용 증가
- 모든 접근마다 타임스탬프 갱신 필요

### 2. FIFO (First In First Out)

**가장 먼저 들어온 블록을 먼저 교체합니다.**

#### 개념

```
아이디어: 큐(Queue) 구조처럼 동작

예시:
시간 0: A 삽입 → [A]
시간 1: B 삽입 → [A, B]
시간 2: C 삽입 → [A, B, C]
시간 3: D 삽입 → [A, B, C, D]

캐시 가득참!

시간 4: E 삽입 필요
→ A 제거 (가장 먼저 들어옴)
→ [B, C, D, E]

시간 5: F 삽입 필요
→ B 제거
→ [C, D, E, F]
```

#### 구현 방법

```c
typedef struct {
    bool valid;
    unsigned int tag;
    unsigned char data[BLOCK_SIZE];
    int insert_time;  // 삽입 시간
} CacheLine;

int global_insert_time = 0;

// FIFO 블록 찾기
int find_fifo_line() {
    int fifo_line = 0;
    int min_time = cache[0].insert_time;
    
    // 가장 작은 삽입 시간을 가진 라인 찾기
    for (int i = 1; i < CACHE_LINES; i++) {
        if (cache[i].insert_time < min_time) {
            min_time = cache[i].insert_time;
            fifo_line = i;
        }
    }
    
    return fifo_line;
}

// 새 블록 삽입 시
void insert_block(int line, unsigned int tag) {
    cache[line].valid = true;
    cache[line].tag = tag;
    cache[line].insert_time = global_insert_time++;
    // 주의: 접근해도 insert_time은 바뀌지 않음!
}
```

**LRU vs FIFO 비교:**

```
접근 패턴: A, B, C, D, A, E

LRU:
시간 0: A → [A]
시간 1: B → [A, B]
시간 2: C → [A, B, C]
시간 3: D → [A, B, C, D]
시간 4: A → [B, C, D, A]  (A 다시 사용, 최신으로)
시간 5: E → [C, D, A, E]  (B 교체, 가장 오래 사용 안함)

FIFO:
시간 0: A → [A]
시간 1: B → [A, B]
시간 2: C → [A, B, C]
시간 3: D → [A, B, C, D]
시간 4: A → [A, B, C, D]  (A 다시 사용, 이미 있음)
시간 5: E → [B, C, D, E]  (A 교체, 먼저 들어왔음!)

→ LRU가 더 효율적!
```

#### 장점과 단점

**장점 ✅**
- 구현이 간단함
- 하드웨어 비용 낮음
- 공평함 (먼저 온 순서대로)

**단점 ❌**
- 자주 사용하는 블록도 교체될 수 있음
- LRU보다 성능 낮음
- 시간 지역성을 활용하지 못함

### 3. Random (무작위)

**임의의 블록을 무작위로 교체합니다.**

#### 구현 방법

```c
#include <stdlib.h>
#include <time.h>

// 초기화
srand(time(NULL));

// 랜덤 블록 선택
int find_random_line() {
    return rand() % CACHE_LINES;
}
```

#### 장점과 단점

**장점 ✅**
- 구현이 매우 간단함
- 하드웨어 비용 최소
- 특정 패턴에 취약하지 않음

**단점 ❌**
- 예측 불가능
- 평균 성능이 낮음
- 운이 나쁘면 자주 사용하는 블록 교체

### 교체 알고리즘 비교

```
성능 순위 (일반적으로):
LRU > FIFO > Random

구현 복잡도:
Random < FIFO < LRU

하드웨어 비용:
Random < FIFO < LRU

실제 사용:
- 고성능 캐시: LRU 또는 LRU 근사 알고리즘
- 중간 성능: FIFO 또는 Random
- 저가형: Random
```

---

## 실전 예제

### 예제 1: LRU 동작 시뮬레이션

```c
#include <stdio.h>
#include <stdbool.h>
#include <string.h>

#define CACHE_LINES 4
#define BLOCK_SIZE 4

typedef struct {
    bool valid;
    unsigned int tag;
    unsigned char data[BLOCK_SIZE];
    int lru_counter;
} CacheLine;

CacheLine cache[CACHE_LINES];
int global_time = 0;
int hits = 0;
int misses = 0;

void init_cache() {
    for (int i = 0; i < CACHE_LINES; i++) {
        cache[i].valid = false;
        cache[i].tag = 0;
        cache[i].lru_counter = 0;
    }
    global_time = 0;
    hits = 0;
    misses = 0;
}

// 태그 검색 (모든 라인 비교)
int find_tag(unsigned int tag) {
    for (int i = 0; i < CACHE_LINES; i++) {
        if (cache[i].valid && cache[i].tag == tag) {
            return i;
        }
    }
    return -1;
}

// 빈 라인 찾기
int find_empty() {
    for (int i = 0; i < CACHE_LINES; i++) {
        if (!cache[i].valid) {
            return i;
        }
    }
    return -1;
}

// LRU 라인 찾기
int find_lru() {
    int lru_line = 0;
    int min_time = cache[0].lru_counter;
    
    for (int i = 1; i < CACHE_LINES; i++) {
        if (cache[i].lru_counter < min_time) {
            min_time = cache[i].lru_counter;
            lru_line = i;
        }
    }
    
    return lru_line;
}

// 캐시 접근
void access_cache(unsigned int block_num) {
    printf("\n블록 %u 접근:\n", block_num);
    
    // 1. 캐시에서 검색
    int line = find_tag(block_num);
    
    if (line != -1) {
        // Hit
        printf("  ✅ Hit! (라인 %d)\n", line);
        cache[line].lru_counter = global_time++;
        hits++;
    } else {
        // Miss
        printf("  ❌ Miss!\n");
        misses++;
        
        // 2. 빈 라인 찾기
        int empty = find_empty();
        if (empty != -1) {
            printf("  → 빈 라인 %d에 저장\n", empty);
            cache[empty].valid = true;
            cache[empty].tag = block_num;
            cache[empty].lru_counter = global_time++;
        } else {
            // 3. LRU 교체
            int victim = find_lru();
            printf("  → 라인 %d 교체 (블록 %u → 블록 %u)\n",
                   victim, cache[victim].tag, block_num);
            cache[victim].tag = block_num;
            cache[victim].lru_counter = global_time++;
        }
    }
    
    // 현재 캐시 상태 출력
    printf("  캐시 상태: [");
    for (int i = 0; i < CACHE_LINES; i++) {
        if (cache[i].valid) {
            printf("%u(LRU:%d)", cache[i].tag, cache[i].lru_counter);
        } else {
            printf("Empty");
        }
        if (i < CACHE_LINES - 1) printf(", ");
    }
    printf("]\n");
}

int main() {
    init_cache();
    
    printf("=== 연관 사상 LRU 시뮬레이션 ===\n");
    printf("캐시 크기: %d 라인\n", CACHE_LINES);
    
    // 접근 패턴
    unsigned int pattern[] = {1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5};
    int pattern_size = sizeof(pattern) / sizeof(pattern[0]);
    
    for (int i = 0; i < pattern_size; i++) {
        access_cache(pattern[i]);
    }
    
    printf("\n=== 통계 ===\n");
    printf("총 접근: %d\n", hits + misses);
    printf("Hit: %d\n", hits);
    printf("Miss: %d\n", misses);
    printf("적중률: %.2f%%\n", (float)hits / (hits + misses) * 100);
    
    return 0;
}
```

**실행 결과:**

```
=== 연관 사상 LRU 시뮬레이션 ===
캐시 크기: 4 라인

블록 1 접근:
  ❌ Miss!
  → 빈 라인 0에 저장
  캐시 상태: [1(LRU:0), Empty, Empty, Empty]

블록 2 접근:
  ❌ Miss!
  → 빈 라인 1에 저장
  캐시 상태: [1(LRU:0), 2(LRU:1), Empty, Empty]

블록 3 접근:
  ❌ Miss!
  → 빈 라인 2에 저장
  캐시 상태: [1(LRU:0), 2(LRU:1), 3(LRU:2), Empty]

블록 4 접근:
  ❌ Miss!
  → 빈 라인 3에 저장
  캐시 상태: [1(LRU:0), 2(LRU:1), 3(LRU:2), 4(LRU:3)]

블록 1 접근:
  ✅ Hit! (라인 0)
  캐시 상태: [1(LRU:4), 2(LRU:1), 3(LRU:2), 4(LRU:3)]

블록 2 접근:
  ✅ Hit! (라인 1)
  캐시 상태: [1(LRU:4), 2(LRU:5), 3(LRU:2), 4(LRU:3)]

블록 5 접근:
  ❌ Miss!
  → 라인 2 교체 (블록 3 → 블록 5)
  캐시 상태: [1(LRU:4), 2(LRU:5), 5(LRU:6), 4(LRU:3)]
  (3이 가장 오래 사용 안함, LRU=2)

블록 1 접근:
  ✅ Hit! (라인 0)
  캐시 상태: [1(LRU:7), 2(LRU:5), 5(LRU:6), 4(LRU:3)]

블록 2 접근:
  ✅ Hit! (라인 1)
  캐시 상태: [1(LRU:7), 2(LRU:8), 5(LRU:6), 4(LRU:3)]

블록 3 접근:
  ❌ Miss!
  → 라인 3 교체 (블록 4 → 블록 3)
  캐시 상태: [1(LRU:7), 2(LRU:8), 5(LRU:6), 3(LRU:9)]
  (4가 가장 오래 사용 안함, LRU=3)

블록 4 접근:
  ❌ Miss!
  → 라인 2 교체 (블록 5 → 블록 4)
  캐시 상태: [1(LRU:7), 2(LRU:8), 4(LRU:10), 3(LRU:9)]
  (5가 가장 오래 사용 안함, LRU=6)

블록 5 접근:
  ❌ Miss!
  → 라인 0 교체 (블록 1 → 블록 5)
  캐시 상태: [5(LRU:11), 2(LRU:8), 4(LRU:10), 3(LRU:9)]
  (1이 가장 오래 사용 안함, LRU=7)

=== 통계 ===
총 접근: 12
Hit: 4
Miss: 8
적중률: 33.33%
```

---

## 장점과 단점

### 장점 ✅

#### 1. 충돌 없음

```
어떤 블록이든 어디든 저장 가능!

직접 사상의 문제:
블록 0, 8, 16 → 모두 라인 0 (충돌!)

연관 사상:
블록 0 → 라인 3
블록 8 → 라인 1
블록 16 → 라인 5
→ 충돌 없음!
```

#### 2. 최대 캐시 활용

```
모든 캐시 공간을 효율적으로 사용

예시:
캐시 8라인, 접근: 블록 0~7

직접 사상:
- 라인 0: 블록 0
- 라인 1: 블록 1
- ...
- 라인 7: 블록 7
→ 100% 활용 (운이 좋은 경우)

연관 사상:
- 어떤 순서든 모두 저장 가능
- 항상 100% 활용 가능
```

#### 3. 높은 적중률

```
LRU 알고리즘 사용 시:
- 자주 사용하는 블록 유지
- 시간 지역성 최대 활용
- 적중률 90% 이상 가능
```

### 단점 ❌

#### 1. 높은 하드웨어 비용

```
필요한 하드웨어:
- 캐시 라인 수만큼의 비교기
- 8라인 = 8개 비교기
- 64라인 = 64개 비교기!

직접 사상:
- 비교기 1개만 필요

비용 차이: 8배~64배!
```

#### 2. 느린 검색 속도

```
모든 라인을 동시에 비교:
- 병렬 비교 회로 필요
- 전력 소비 증가
- 발열 증가

특히 캐시가 크면:
- 64라인 이상은 비현실적
- 배선 복잡도 증가
- 신호 지연 발생
```

#### 3. 복잡한 교체 로직

```
LRU 구현:
- 각 라인의 타임스탬프 저장
- 매 접근마다 갱신
- 교체 시 모든 타임스탬프 비교

추가 하드웨어:
- 카운터
- 비교 로직
- 갱신 로직
```

---

## 실무 활용 사례

### 1. TLB (Translation Lookaside Buffer)

```
특징:
- 가상 주소 → 물리 주소 변환 캐시
- 크기: 16~512 엔트리 (작음!)
- 완전 연관 사상 또는 집합 연관 사상

이유:
- 크기가 작아서 연관 사상 가능
- 적중률이 매우 중요 (99% 이상)
- 변환 속도가 중요
```

### 2. 특수 목적 캐시

```
예시:
- 브랜치 예측 테이블
- 프리페치 버퍼
- 쓰기 버퍼

공통점:
- 작은 크기 (수십 엔트리)
- 높은 적중률 필요
- 유연한 매핑 필요
```

### 3. 고성능 프로세서의 L1 캐시

```
일부 고성능 프로세서:
- 매우 작은 L1 I-Cache (16KB)
- 연관 사상 또는 높은 웨이 집합 연관
- 예: 8-Way, 16-Way

목적:
- 최대 적중률
- 예측 가능한 성능
```

---

## 연관 사상 vs 직접 사상 비교

### 성능 비교 시뮬레이션

```c
// 같은 접근 패턴으로 두 방식 비교
unsigned int pattern[] = {0, 8, 16, 0, 8, 16, 24, 0, 8};

// 직접 사상 (8라인):
// 블록 0, 8, 16, 24 모두 라인 0에 매핑
// 결과: 계속 교체, 적중률 0%

// 연관 사상 (8라인):
// 블록 0 → 라인 0
// 블록 8 → 라인 1
// 블록 16 → 라인 2
// 블록 24 → 라인 3
// 결과: 충돌 없음, 적중률 높음
```

### 종합 비교표

| 특성 | 직접 사상 | 연관 사상 |
|------|----------|----------|
| **매핑 규칙** | 고정 (블록 % 라인) | 없음 (자유) |
| **검색 방법** | 인덱스 직접 접근 | 모든 라인 비교 |
| **비교 횟수** | 1회 | 전체 라인 수 |
| **비교기 수** | 1개 | 라인 수만큼 |
| **태그 크기** | 작음 | 큼 |
| **충돌** | 많음 | 없음 |
| **적중률** | 낮음~중간 | 높음 |
| **하드웨어 비용** | 낮음 | 높음 |
| **속도** | 빠름 | 느림 (병렬 비교 필요) |
| **전력 소비** | 낮음 | 높음 |
| **확장성** | 좋음 | 나쁨 (라인 수 증가 시) |
| **교체 알고리즘** | 불필요 | 필수 (LRU, FIFO 등) |

---

## 핵심 정리

### 연관 사상의 특징

```
✅ 장점:
1. 충돌 없음 (자유로운 배치)
2. 최대 캐시 활용
3. 높은 적중률 (LRU 사용 시)

❌ 단점:
1. 높은 하드웨어 비용 (다수의 비교기)
2. 느린 검색 (병렬 비교 필요)
3. 복잡한 교체 로직
```

### 주소 구조

```
┌──────────────┬────────┐
│     태그      │ 오프셋  │
└──────────────┴────────┘

※ 인덱스 없음!
태그 = 전체 비트 - 오프셋 비트
```

### 교체 알고리즘

```
LRU (Least Recently Used):
- 가장 오래 사용 안한 블록 교체
- 성능 최고, 구현 복잡

FIFO (First In First Out):
- 가장 먼저 들어온 블록 교체
- 성능 중간, 구현 간단

Random:
- 무작위 교체
- 성능 낮음, 구현 매우 간단
```

---

## 연습 문제

### 문제 1

16비트 주소, 블록 크기 16바이트, 캐시 라인 32개인 연관 사상에서:
1. 태그와 오프셋의 비트 수는?
2. 주소 0xABCD의 태그와 오프셋은?

<details>
<summary>답안 보기</summary>

1. 비트 수:
   - 오프셋: log₂(16) = 4비트
   - 인덱스: 0비트 (연관 사상)
   - 태그: 16 - 4 = 12비트

2. 0xABCD = 1010 1011 1100 1101
   - 태그: 1010 1011 1100 (상위 12비트) = 2748
   - 오프셋: 1101 (하위 4비트) = 13

</details>

### 문제 2

LRU 알고리즘으로 4라인 캐시에 접근 패턴 A, B, C, D, A, E, F, A가 주어질 때:
1. 각 접근의 Hit/Miss는?
2. 최종 캐시 상태는?

<details>
<summary>답안 보기</summary>

1. 접근 결과:
   - A: Miss (라인 0에 저장)
   - B: Miss (라인 1에 저장)
   - C: Miss (라인 2에 저장)
   - D: Miss (라인 3에 저장)
   - A: Hit (라인 0, LRU 갱신)
   - E: Miss (D 교체, 라인 3)
   - F: Miss (C 교체, 라인 2)
   - A: Hit (라인 0)

2. 최종 캐시: [A, B, F, E]
   LRU 순서: A(최신) > F > E > B(가장 오래됨)

</details>

---

## 다음 단계

연관 사상의 개념을 이해했다면:

- **집합 연관 사상**: 직접 사상과 연관 사상의 장점을 결합한 현실적인 해결책
- **캐시 일관성**: 멀티코어 시스템에서의 캐시 문제
- **실제 CPU 구조**: Intel, AMD CPU의 실제 캐시 구조 분석

다음 문서에서 현대 CPU가 실제로 사용하는 **집합 연관 사상**을 배워봅시다!